{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f74c3f5",
   "metadata": {},
   "source": [
    "To import the data from BQ we have used the tutorial \"Visualizing BigQuery public data that was located in the tutorials folder of repo.\n",
    "\n",
    "Additional modifications have been made to filter and remove duplicates in the data.\n",
    "\n",
    "Information about the sentiment classification library can be found here: https://huggingface.co/marma/bert-base-swedish-cased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87032f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (precision_recall_fscore_support)\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614252b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the labeled comments that are labeled 1 or -1\n",
    "sql_labeled = \"\"\"\n",
    "SELECT * FROM `BQ TABLE NAME` WHERE NOT sentiment_label=0\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Pre-processing of the labeled data\n",
    "'''\n",
    "dfs = client.query(sql_labeled).to_dataframe() \n",
    "fb_comments_labeled_df = pd.concat(dfs)\n",
    "fb_comments_labeled_df = fb_comments_labeled_df.drop_duplicates(subset='comment_id')\n",
    "\n",
    "fb_comments_labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14373262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(targets, predictions):\n",
    "    scores = precision_recall_fscore_support(predictions, targets, average=None)\n",
    "    p_r_f_for_class = dict()\n",
    "    for i in range(2):\n",
    "        label = 'Positive'\n",
    "        if i == 0:\n",
    "            label = 'Negative'\n",
    "            \n",
    "        p_r_f_for_class[label] = (scores[0][i], scores[1][i], scores[2][i])\n",
    "    return  p_r_f_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bb230b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  280  comments in the test set\n",
      "There are  982  labeled comments total\n",
      "Negative: 74% (728)\n",
      "Positive: 25% (254)\n"
     ]
    }
   ],
   "source": [
    "# Load the test set\n",
    "test_df = pd.read_pickle(\"./test_labeled_data.pkl\")\n",
    "all_labeled_df = pd.read_pickle(\"./labeled_filtered_comments.pkl\")\n",
    "\n",
    "# Set the labels to be 1 or -1\n",
    "test_df['sentiment_label'] = test_df['sentiment_label'].map({'pos':1, 'neg': 0})\n",
    "all_labeled_df['sentiment_label'] = all_labeled_df['sentiment_label'].map({'pos':1, 'neg': 0})\n",
    "\n",
    "print(\"There are \", len(test_df), \" comments in the test set\")\n",
    "print(\"There are \", len(all_labeled_df), \" labeled comments total\")\n",
    "n_neg, n_pos = all_labeled_df[\"sentiment_label\"].value_counts()[0], all_labeled_df[\"sentiment_label\"].value_counts()[1]\n",
    "print(\"Negative: %d%% (%d)\" %(n_neg*100/len(all_labeled_df), n_neg))\n",
    "print(\"Positive: %d%% (%d)\" %(n_pos*100/len(all_labeled_df), n_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8452ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = pipeline('sentiment-analysis', model='marma/bert-base-swedish-cased-sentiment')\n",
    "\n",
    "targets = list(all_labeled_df['sentiment_label'].values)\n",
    "features = list(all_labeled_df['message'].values)\n",
    "preds = []\n",
    "\n",
    "# classify all texts in the labeled dataset\n",
    "for text, label in zip(features, targets):\n",
    "    # print('text', text)\n",
    "    # print('label', label)\n",
    "    if len(text) < 512:\n",
    "        pred = sa(text)\n",
    "        label = pred[0]['label']\n",
    "        if label == 'NEGATIVE':\n",
    "            preds.append(0)\n",
    "        else:\n",
    "            preds.append(1)\n",
    "    else:\n",
    "        preds.append(0) # classify as negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aa648ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  {'Negative': (0.8832417582417582, 0.9455882352941176, 0.9133522727272727), 'Positive': (0.8543307086614174, 0.7185430463576159, 0.7805755395683454)}\n",
      "Accuracy: 0.876\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the predicitons\n",
    "\n",
    "scores = calculate_precision_recall(targets, preds)\n",
    "print('scores: ', scores)\n",
    "count = 0\n",
    "for i, (pred, target) in enumerate(zip(preds, targets)):\n",
    "    if pred == target:\n",
    "        count += 1\n",
    "    # else:\n",
    "        # print('Predicted label: ', pred)\n",
    "        # print('Target label: ', target)\n",
    "        # print('Message: ', features[i])\n",
    "        \n",
    "test_accuracy = count / len(preds)\n",
    "print(\"Accuracy: {0:.3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m79"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
