{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83149ff",
   "metadata": {},
   "source": [
    "# 1. Load the labeled and unlabeled comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# !pip install transformers==4.3.2\n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from typing import Union\n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install sentencepiece\n",
    "\n",
    "##Set random values\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97df36e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dc5c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have  6794  unlabeled comments\n",
      "You're using:  959  labeled comments.\n",
      "Negative: 74% (710)\n",
      "Positive: 25% (249)\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered unlabeled comments and all the labeled comments.\n",
    "fb_comments_unlabeled_df = pd.read_pickle(\"./unlabeled_filtered_comments.pkl\")\n",
    "fb_comments_labeled_df = pd.read_pickle(\"./labeled_comments.pkl\")\n",
    "\n",
    "print(\"You have \", len(fb_comments_unlabeled_df), \" unlabeled comments\")\n",
    "print(\"You're using: \", len(fb_comments_labeled_df), \" labeled comments.\")\n",
    "n_neg, n_pos = fb_comments_labeled_df[\"sentiment_label\"].value_counts()[0], fb_comments_labeled_df[\"sentiment_label\"].value_counts()[1]\n",
    "print(\"Negative: %d%% (%d)\" %(n_neg*100/len(fb_comments_labeled_df), n_neg))\n",
    "print(\"Positive: %d%% (%d)\" %(n_pos*100/len(fb_comments_labeled_df), n_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc25a4",
   "metadata": {},
   "source": [
    "# 2. Get train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fd15af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using:  476  labeled comments.\n",
      "\n",
      "Percentages in train:\n",
      "Negative: 73% (346)\n",
      "Positive:  27% (130)\n",
      "\n",
      "Percentages in test: \n",
      "Negative: 74%\n",
      "Positive:  26%\n"
     ]
    }
   ],
   "source": [
    "def get_train_test(labeled_comments: pd.core.frame.DataFrame, test_size: Union[int, float],\n",
    "                   new_split: bool = False, files: dict = {}) -> tuple:\n",
    "    if new_split:\n",
    "        if isinstance(test_size, int):\n",
    "            test_size = test_size / labeled_comments.shape[0]\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state = 0)\n",
    "        sss.get_n_splits(labeled_comments[\"message\"].values, labeled_comments[\"sentiment_label\"].values)\n",
    "        train_labeled_data = None\n",
    "        test_labeled_data = None\n",
    "        for train_index, test_index in sss.split(labeled_comments[\"message\"].values, labeled_comments[\"sentiment_label\"].values):\n",
    "            train_labeled_data = labeled_comments.iloc[train_index]\n",
    "            test_labeled_data = labeled_comments.iloc[test_index]\n",
    "        train_labeled_data.to_pickle(\"./train_labeled_data.pkl\")\n",
    "        test_labeled_data.to_pickle(\"./test_labeled_data.pkl\")\n",
    "    else:\n",
    "        train_labeled_data = pd.read_pickle(files[\"train\"])\n",
    "        test_labeled_data = pd.read_pickle(files[\"test\"])\n",
    "    \n",
    "    return train_labeled_data, test_labeled_data\n",
    "        \n",
    "\n",
    "             \n",
    "def prepare_data(train_labeled_comments: pd.core.frame.DataFrame, test_labeled_comments: pd.core.frame.DataFrame,\n",
    "                 unlabeled_comments: pd.core.frame.DataFrame, percentage: Union[int, float],\n",
    "                display_class_percentages: bool = False) -> tuple:\n",
    "    \n",
    "    # Grab the requested percentage (with respect to unlabeled data) of labeled comments for training. \n",
    "    if isinstance(percentage, int):\n",
    "        percentage = percentage/100\n",
    "    if percentage != .1:\n",
    "        labeled_comments = train_labeled_comments.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "        n_labeled = train_labeled_comments.shape[0]\n",
    "        n_unlabeled = unlabeled_comments.shape[0]\n",
    "        n_wanted = round(percentage * n_unlabeled)\n",
    "        if n_wanted > n_labeled:\n",
    "            raise Exception(\"ERROR. Insufficient amount of labeled comments, try a lower percentage.\")\n",
    "        n_drop = n_labeled - n_wanted\n",
    "        train_labeled_comments.drop(train_labeled_comments.tail(n_drop).index, inplace=True)\n",
    "        print(\"You're using: \", train_labeled_comments.shape[0], \" labeled comments.\\n\")\n",
    "    \n",
    "    # Prepare the data for the DataLoader function below.\n",
    "    # 1. Make tuples with labeled data: (feature, label). \n",
    "    unlabeled_arr = np.array([(message, \"UNK_UNK\") for message in unlabeled_comments[\"message\"].values])\n",
    "    train_arr = np.array([(row[\"message\"], row[\"sentiment_label\"]) for _, row in train_labeled_comments.iterrows()])\n",
    "    test_arr = np.array([(row[\"message\"], row[\"sentiment_label\"]) for _, row in test_labeled_comments.iterrows()])\n",
    "    \n",
    "    if display_class_percentages: \n",
    "        # Use code of previous versions (ugly).\n",
    "        train_labeled_data = train_arr\n",
    "        d_train = {\"pos\": 0, \"neg\": 0, \"UNK_UNK\": 0}\n",
    "        d_test = d_train.copy()\n",
    "        for _, label in train_labeled_data:\n",
    "            d_train[label] += 1\n",
    "        print(\"Percentages in train:\")\n",
    "        print(\"Negative: %d%% (%d)\" %(round(d_train[\"neg\"]*100/len(train_labeled_data)), d_train[\"neg\"]))\n",
    "        print(\"Positive:  %d%% (%d)\" %(round(d_train[\"pos\"]*100/len(train_labeled_data)), d_train[\"pos\"]))\n",
    "        print(\"\")\n",
    "\n",
    "        test_labeled_data = test_arr\n",
    "        for _, label in test_labeled_data:\n",
    "            d_test[label] += 1\n",
    "        print(\"Percentages in test: \")\n",
    "        print(\"Negative: %d%%\" %(round(d_test[\"neg\"]*100/len(test_labeled_data))))\n",
    "        print(\"Positive:  %d%%\" %(round(d_test[\"pos\"]*100/len(test_labeled_data))))\n",
    "        \n",
    "    # 2. Create mask arrays.\n",
    "    unlabeled_masks = np.zeros(unlabeled_arr.shape[0], dtype=bool)\n",
    "    train_masks = np.ones(train_arr.shape[0], dtype=bool)\n",
    "    test_masks = np.ones(test_arr.shape[0], dtype=bool)\n",
    "    # 3. Extend the train data with the unlabeled data.\n",
    "    train_arr = np.concatenate((train_arr, unlabeled_arr), axis=0)\n",
    "    train_masks = np.concatenate((train_masks, unlabeled_masks))\n",
    "    \n",
    "    \n",
    "    return train_arr, train_masks, test_arr, test_masks\n",
    "\n",
    "new_split = False\n",
    "test_size = 280\n",
    "files = {\"train\": \"train_labeled_data.pkl\", \"test\": \"test_labeled_data.pkl\"}\n",
    "train_labeled_data, test_labeled_data = get_train_test(fb_comments_labeled_df, test_size, new_split=new_split, files=files)\n",
    "labeled_data_percentage = .07 # Used 1, 3, 5, 10 % for linear lr experiment\n",
    "display = True\n",
    "train_examples, train_label_masks, test_examples, test_label_masks = prepare_data(train_labeled_data, \n",
    "                                                                                  test_labeled_data,\n",
    "                                                                                  fb_comments_unlabeled_df,\n",
    "                                                                                  labeled_data_percentage,\n",
    "                                                                                 display_class_percentages=display)\n",
    "\n",
    "#print(\"Now your're using: \", len(train_examples), \" labeled comments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e951669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98168bd1",
   "metadata": {},
   "source": [
    "# 3.1 Architecture parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef7a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf7023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"neg\", \"pos\", \"UNK_UNK\"]\n",
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size = 32\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 5\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = True # CHANGED THIS\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "model_name = \"KB/bert-base-swedish-cased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d4d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_files_only = True\n",
    "transformer = AutoModel.from_pretrained(model_name, local_files_only=local_files_only)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=local_files_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba59049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/src'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455fc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b83816",
   "metadata": {},
   "source": [
    "# 3.2 DataLoader function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871aabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, batch_size, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples\n",
    "  label_mask_rate = sum(label_masks)/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  print(len(examples))\n",
    "  print(len(text[0]))\n",
    "  print(len(encoded_sent))\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "            dataset,  # The training samples.\n",
    "            sampler = sampler(dataset),\n",
    "            drop_last = True,\n",
    "            batch_size = batch_size) # Trains with this batch size.\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d57a68",
   "metadata": {},
   "source": [
    "# 4. Training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b75b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3a574",
   "metadata": {},
   "source": [
    "# 4. Training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe47dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Helper fucntions for the training procedure\n",
    "#--------------------------------\n",
    "from sklearn.metrics import (PrecisionRecallDisplay, precision_recall_fscore_support, precision_recall_curve)\n",
    "\n",
    "# Precision = TP/(TP+FP) Ratio between true positives and all positived\n",
    "# Recall = TP/(TP+FN) How well does the model identify true positives\n",
    "def calculate_precision_recall(all_preds, all_label_ids):\n",
    "    scores = precision_recall_fscore_support(all_preds, all_label_ids, average=None)\n",
    "    p_r_f_for_class = dict()\n",
    "    for i in range(2):\n",
    "        label = 'Positive' if i != 0 else \"Negative\"\n",
    "        p_r_f_for_class[label] = (scores[0][i], scores[1][i], scores[2][i])\n",
    "\n",
    "    print('precision / recall / f-score for class: ', p_r_f_for_class)\n",
    "    precision_arr, recall_arr, _ = precision_recall_curve(all_label_ids, all_preds)\n",
    "    \n",
    "    return p_r_f_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10dcfa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUN NUMBER:  1\n",
      "8222\n",
      "66\n",
      "64\n",
      "280\n",
      "87\n",
      "64\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:22.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:43.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:04.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:36.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:57.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:18.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:39.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:03:00.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:11.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:21.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:32.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:53.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:14.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.655\n",
      "  Average training loss discriminator: 1.300\n",
      "  Training epcoh took: 0:04:31\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9738219895287958, 0.8611111111111112, 0.9140049140049141), 'Positive': (0.5384615384615384, 0.875, 0.6666666666666667)}\n",
      "  Accuracy: 0.863\n",
      "  Test Loss: 0.415\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:39.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:03:00.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:21.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.714\n",
      "  Average training loss discriminator: 0.779\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9581151832460733, 0.9242424242424242, 0.9408740359897172), 'Positive': (0.7692307692307693, 0.8620689655172413, 0.8130081300813008)}\n",
      "  Accuracy: 0.910\n",
      "  Test Loss: 0.353\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.740\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9267015706806283, 0.9619565217391305, 0.9440000000000001), 'Positive': (0.8923076923076924, 0.8055555555555556, 0.8467153284671534)}\n",
      "  Accuracy: 0.918\n",
      "  Test Loss: 0.366\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:21.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.707\n",
      "  Average training loss discriminator: 0.722\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9581151832460733, 0.9481865284974094, 0.953125), 'Positive': (0.8461538461538461, 0.873015873015873, 0.859375)}\n",
      "  Accuracy: 0.930\n",
      "  Test Loss: 0.330\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:57.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:18.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:39.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:03:00.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:11.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:21.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:32.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:43.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:53.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:04.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:14.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:25.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 0.718\n",
      "  Training epcoh took: 0:04:31\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9528795811518325, 0.9528795811518325, 0.9528795811518325), 'Positive': (0.8615384615384616, 0.8615384615384616, 0.8615384615384615)}\n",
      "  Accuracy: 0.930\n",
      "  Test Loss: 0.335\n",
      "  Test took: 0:00:02\n",
      "\n",
      "RUN NUMBER:  2\n",
      "8222\n",
      "66\n",
      "64\n",
      "280\n",
      "87\n",
      "64\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.708\n",
      "  Average training loss discriminator: 0.758\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9581151832460733, 0.9104477611940298, 0.9336734693877551), 'Positive': (0.7230769230769231, 0.8545454545454545, 0.7833333333333332)}\n",
      "  Accuracy: 0.898\n",
      "  Test Loss: 0.414\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:06.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:27.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:48.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:09.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:30.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:51.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:02.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:12.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss generetor: 0.705\n",
      "  Average training loss discriminator: 0.733\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9267015706806283, 0.946524064171123, 0.9365079365079365), 'Positive': (0.8461538461538461, 0.7971014492753623, 0.8208955223880597)}\n",
      "  Accuracy: 0.906\n",
      "  Test Loss: 0.712\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.707\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.93717277486911, 0.93717277486911, 0.93717277486911), 'Positive': (0.8153846153846154, 0.8153846153846154, 0.8153846153846154)}\n",
      "  Accuracy: 0.906\n",
      "  Test Loss: 0.823\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:21.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.701\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.93717277486911, 0.93717277486911, 0.93717277486911), 'Positive': (0.8153846153846154, 0.8153846153846154, 0.8153846153846154)}\n",
      "  Accuracy: 0.906\n",
      "  Test Loss: 0.872\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.93717277486911, 0.93717277486911, 0.93717277486911), 'Positive': (0.8153846153846154, 0.8153846153846154, 0.8153846153846154)}\n",
      "  Accuracy: 0.906\n",
      "  Test Loss: 0.880\n",
      "  Test took: 0:00:02\n",
      "\n",
      "RUN NUMBER:  3\n",
      "8222\n",
      "66\n",
      "64\n",
      "280\n",
      "87\n",
      "64\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:42.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.709\n",
      "  Average training loss discriminator: 0.902\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9162303664921466, 0.8928571428571429, 0.9043927648578811), 'Positive': (0.676923076923077, 0.7333333333333333, 0.7040000000000001)}\n",
      "  Accuracy: 0.855\n",
      "  Test Loss: 0.486\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:25.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:46.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:07.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:28.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:49.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:10.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:31.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:52.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:03.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:24.\n",
      "\n",
      "  Average training loss generetor: 0.704\n",
      "  Average training loss discriminator: 0.791\n",
      "  Training epcoh took: 0:04:30\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9319371727748691, 0.9128205128205128, 0.9222797927461139), 'Positive': (0.7384615384615385, 0.7868852459016393, 0.7619047619047619)}\n",
      "  Accuracy: 0.883\n",
      "  Test Loss: 0.523\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:06.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:27.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:48.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:09.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:30.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:51.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:02.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:12.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss generetor: 0.703\n",
      "  Average training loss discriminator: 0.730\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9214659685863874, 0.946236559139785, 0.9336870026525199), 'Positive': (0.8461538461538461, 0.7857142857142857, 0.8148148148148148)}\n",
      "  Accuracy: 0.902\n",
      "  Test Loss: 0.588\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:31.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:52.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:13.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:34.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:06.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:27.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:48.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:09.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:30.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:51.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:02.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:12.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.705\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9162303664921466, 0.9459459459459459, 0.9308510638297872), 'Positive': (0.8461538461538461, 0.7746478873239436, 0.8088235294117647)}\n",
      "  Accuracy: 0.898\n",
      "  Test Loss: 0.650\n",
      "  Test took: 0:00:02\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    10  of    256.    Elapsed: 0:00:11.\n",
      "  Batch    20  of    256.    Elapsed: 0:00:21.\n",
      "  Batch    30  of    256.    Elapsed: 0:00:32.\n",
      "  Batch    40  of    256.    Elapsed: 0:00:42.\n",
      "  Batch    50  of    256.    Elapsed: 0:00:53.\n",
      "  Batch    60  of    256.    Elapsed: 0:01:03.\n",
      "  Batch    70  of    256.    Elapsed: 0:01:14.\n",
      "  Batch    80  of    256.    Elapsed: 0:01:24.\n",
      "  Batch    90  of    256.    Elapsed: 0:01:35.\n",
      "  Batch   100  of    256.    Elapsed: 0:01:45.\n",
      "  Batch   110  of    256.    Elapsed: 0:01:56.\n",
      "  Batch   120  of    256.    Elapsed: 0:02:06.\n",
      "  Batch   130  of    256.    Elapsed: 0:02:17.\n",
      "  Batch   140  of    256.    Elapsed: 0:02:27.\n",
      "  Batch   150  of    256.    Elapsed: 0:02:38.\n",
      "  Batch   160  of    256.    Elapsed: 0:02:48.\n",
      "  Batch   170  of    256.    Elapsed: 0:02:59.\n",
      "  Batch   180  of    256.    Elapsed: 0:03:09.\n",
      "  Batch   190  of    256.    Elapsed: 0:03:20.\n",
      "  Batch   200  of    256.    Elapsed: 0:03:30.\n",
      "  Batch   210  of    256.    Elapsed: 0:03:41.\n",
      "  Batch   220  of    256.    Elapsed: 0:03:51.\n",
      "  Batch   230  of    256.    Elapsed: 0:04:02.\n",
      "  Batch   240  of    256.    Elapsed: 0:04:13.\n",
      "  Batch   250  of    256.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss generetor: 0.702\n",
      "  Average training loss discriminator: 0.704\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Test...\n",
      "256\n",
      "precision / recall / f-score for class:  {'Negative': (0.9162303664921466, 0.9459459459459459, 0.9308510638297872), 'Positive': (0.8461538461538461, 0.7746478873239436, 0.8088235294117647)}\n",
      "  Accuracy: 0.898\n",
      "  Test Loss: 0.655\n",
      "  Test took: 0:00:02\n",
      "Data saved! Took me 4082.88 seconds.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "save_data = True\n",
    "percentage = str(labeled_data_percentage*100) if isinstance(labeled_data_percentage, float) else str(labeled_data_percentage)\n",
    "st = time.time()\n",
    "for i in range(3):\n",
    "    print(\"\\nRUN NUMBER: \", i+1)\n",
    "    \n",
    "    train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, \n",
    "                                            batch_size, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "    test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, \n",
    "                                       batch_size, do_shuffle = False, balance_label_examples = False)\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    #models parameters\n",
    "    transformer_vars = [i for i in transformer.parameters()]\n",
    "    d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "    g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "    #optimizer\n",
    "    dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "    gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "    #scheduler\n",
    "    if apply_scheduler:\n",
    "      num_train_examples = len(train_examples)\n",
    "      num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "      num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "    \n",
    "      # USE LINEAR LEARNING RATE IN THIS EXPERIMENT!\n",
    "      scheduler_d = get_linear_schedule_with_warmup(dis_optimizer, \n",
    "                                               num_warmup_steps = num_warmup_steps, num_training_steps=num_train_steps)\n",
    "      scheduler_g = get_linear_schedule_with_warmup(gen_optimizer, \n",
    "                                               num_warmup_steps = num_warmup_steps, num_training_steps=num_train_steps)\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, num_train_epochs):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        # Perform one full pass over the training set.\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        tr_g_loss = 0\n",
    "        tr_d_loss = 0\n",
    "\n",
    "        # Put the model into training mode.\n",
    "        transformer.train() \n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every print_each_n_step batches.\n",
    "            if step % print_each_n_step == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            b_label_mask = batch[3].to(device)\n",
    "\n",
    "            # Encode real data in the Transformer\n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "\n",
    "            # Generate fake data that should have the same distribution of the ones\n",
    "            # encoded by the transformer. \n",
    "            # First noisy input are used in input to the Generator\n",
    "            noise = torch.zeros(b_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n",
    "            # Gnerate Fake data\n",
    "            gen_rep = generator(noise)\n",
    "\n",
    "            # Generate the output of the Discriminator for real and fake data.\n",
    "            # First, we put together the output of the tranformer and the generator\n",
    "            disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "            # Then, we select the output of the disciminator\n",
    "            features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "            # Finally, we separate the discriminator's output for the real and fake\n",
    "            # data\n",
    "            features_list = torch.split(features, batch_size)\n",
    "            D_real_features = features_list[0]\n",
    "            D_fake_features = features_list[1]\n",
    "\n",
    "            logits_list = torch.split(logits, batch_size)\n",
    "            D_real_logits = logits_list[0]\n",
    "            D_fake_logits = logits_list[1]\n",
    "\n",
    "            probs_list = torch.split(probs, batch_size)\n",
    "            D_real_probs = probs_list[0]\n",
    "            D_fake_probs = probs_list[1]\n",
    "\n",
    "            #---------------------------------\n",
    "            #  LOSS evaluation\n",
    "            #---------------------------------\n",
    "            # Generator's LOSS estimation\n",
    "            g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "            g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "            g_loss = g_loss_d + g_feat_reg\n",
    "\n",
    "            # Disciminator's LOSS estimation\n",
    "            logits = D_real_logits[:,0:-1]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            # The discriminator provides an output for labeled and unlabeled real data\n",
    "            # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "            label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "            per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "            per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "            labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "            # It may be the case that a batch does not contain labeled examples, \n",
    "            # so the \"supervised loss\" in this case is not evaluated\n",
    "            if labeled_example_count == 0:\n",
    "              D_L_Supervised = 0\n",
    "            else:\n",
    "              D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "\n",
    "            D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "            D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "            d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "            #---------------------------------\n",
    "            #  OPTIMIZATION\n",
    "            #---------------------------------\n",
    "            # Avoid gradient accumulation\n",
    "            gen_optimizer.zero_grad()\n",
    "            dis_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate weigth updates\n",
    "            # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            d_loss.backward() \n",
    "\n",
    "            # Apply modifications\n",
    "            gen_optimizer.step()\n",
    "            dis_optimizer.step()\n",
    "\n",
    "            # A detail log of the individual losses\n",
    "            #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "            #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "            #             g_loss_d, g_feat_reg))\n",
    "\n",
    "            # Save the losses to print them later\n",
    "            tr_g_loss += g_loss.item()\n",
    "            tr_d_loss += d_loss.item()\n",
    "\n",
    "            # Update the learning rate with the scheduler\n",
    "            if apply_scheduler:\n",
    "              scheduler_d.step()\n",
    "              scheduler_g.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "        avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "        print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #     TEST ON THE EVALUATION DATASET\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our test set.\n",
    "        print(\"\")\n",
    "        print(\"Running Test...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        transformer.eval() #maybe redundant\n",
    "        discriminator.eval()\n",
    "        generator.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_test_accuracy = 0\n",
    "\n",
    "        total_test_loss = 0\n",
    "        nb_test_steps = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels_ids = []\n",
    "\n",
    "        #loss\n",
    "        nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs[-1]\n",
    "                _, logits, probs = discriminator(hidden_states)\n",
    "                ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "                filtered_logits = logits[:,0:-1]\n",
    "                # Accumulate the test loss.\n",
    "                total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "\n",
    "            # Accumulate the predictions and the input labels\n",
    "            _, preds = torch.max(filtered_logits, 1)\n",
    "            all_preds += preds.detach().cpu()\n",
    "            all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        all_preds = torch.stack(all_preds).numpy()\n",
    "        print(len(all_preds))\n",
    "        all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "        test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "        p_r_f_for_class = calculate_precision_recall(all_preds, all_labels_ids)\n",
    "        print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        test_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "        print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss generator': avg_train_loss_g,\n",
    "                'Training Loss discriminator': avg_train_loss_d,\n",
    "                'Valid. Loss': avg_test_loss,\n",
    "                'Valid. Accur.': test_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Test Time': test_time,\n",
    "                'PRF': p_r_f_for_class\n",
    "            }\n",
    "        )\n",
    "    data.append(training_stats)\n",
    "    \n",
    "if save_data:\n",
    "    print(\"Data saved! Took me %.2f seconds.\" %(time.time()-st))\n",
    "    np.save(\"linear_lr_results_\" + percentage + \"_percent\", \n",
    "            data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"results_linear_lr_10_percent\", data)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m79"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
